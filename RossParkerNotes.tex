\documentclass[]{article}

\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}

\usepackage{mathtools}
\usepackage{cool}
\usepackage{graphicx}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{assumption}
\newtheorem{assumption}{Assumption}

\theoremstyle{remark}
\newtheorem*{question}{Question}
\newtheorem*{observation}{Observation}
\newtheorem*{remark}{Remark}

\setlength{\parindent}{0cm}

\begin{document}

\title{Notes on MLMC schemes for Reflected Diffusions}
\author{Ross Parker}
\maketitle

\section{Bounds on solutions to reflected SDEs}

\subsection{Preliminaries}
Consider the reflected SDE on an open domain $U$ in $\mathbb{R}^d$:
\[
dX_t = b(X_t, t)dt + \sigma(X_t, t)dW_t + \nu(X_t)dL_t
\]
Where $W_t = (W_1(t), ..., W_d(t) )$ is a $d$-dimensional Brownian motion, $\nu$ is an oblique reflection vector on the boundary of $U$, and $L_t$ is the local time on the boundary. We will consider the solution on the closed interval $[0, T]$. For simplicity of notation, we will take $b$ and $\sigma$ to be independent of $t$, in which case the SDE reduces to:
\begin{equation}
dX_t = b(X_t)dt + \sigma(X_t)dW_t + \nu(X_t)dL_t
\end{equation}
where $b: \mathbb{R}^d \rightarrow \mathbb{R}^d$ and $\sigma: \mathbb{R}^d \rightarrow \mathbb{R}^d \times  \mathbb{R}^d$. In coordinate form, we have for $i = 1, ..., d$:
\begin{equation}
dX_i(t) = b_i(X_t)dt + \sum_{j = 1}^d \sigma_{ij}(X_t)dW_j(t) + \nu_i(X_t)dL_i(t)
\end{equation} 
We are trying to prove the following proposition (variation of Lemma 5.1 on p.9 of the Giles paper):
\begin{proposition}For all $t \leq T$ there exist constants $c$ and $K(T)$ such that:
\begin{equation}
\mathbb{P}\left(\sup_{[0, t]} \norm{X_t} \geq \eta \right) \leq K(T)exp\left( -\frac{c \eta^2}{t}  \right)
\end{equation}
\end{proposition}
We make the following standard assumptions on the coefficient functions $b$ and $\sigma$:

\begin{assumption}The coefficient functions are Lipschitz and satisfy a growth condition, i.e. there exist constants $K_1$ and $K_2$ such that for all $x$ and $y$: 

\begin{equation}
\abs{b(x) - b(y)}  \leq K_1 \abs{x - y}
\end{equation}
\begin{equation}
\norm{\sigma(x) - \sigma(y)} \leq K_1 \abs{x - y}
\end{equation}
\begin{equation}
\abs{b(x)}^2 \leq K_2(1 + \abs{x}^2 )
\end{equation}
\begin{equation}
\norm{\sigma(x)}^2 \leq K_2(1 + \abs{x}^2 )
\end{equation}
In addition, for simplicity we will assume that the initial condition is $X(0) = 0$ a.s. \\ \\
\end{assumption}
Now write the SDE in integrated form. Recalling that we are starting at 0 a.s.:
\[
X(t) = \int_0^t b(X_s)ds + \int_0^t  \sigma(X_s)dW_s + \int_0^t \nu(X_s)dL_s
\]
X(t) is continuous and is confined to the region $U$. The final term on the RHS is nonnegative and increases only on the boundary of $U$. Thus X(t) is the unique solution to the Skorokhod problem for:
\[
H(t) =  \int_0^t b(X_s)ds + \int_0^t  \sigma(X_s)dW_s 
\]
Letting $\Gamma: C[0, T] \rightarrow C[0, T]$ be the Skorokhod mapping, we have $X = \Gamma[H]$. Substituting this above, we obtain the functional SDE for H:
\begin{equation}
H(t) =  \int_0^t b(\Gamma(H)(s))ds + \int_0^t  \sigma(\Gamma(H)(s))dW_s 
\end{equation}
In component form, this is:
\begin{equation}
H_i(t) =  \underbrace{ \int_0^t b_i(\Gamma(H)(s))ds }_{A_i(t)}+ \underbrace{\sum_{j=1}^d \int_0^t  \sigma_{ij}(\Gamma(H)(s))dW_s }_{M_i(t)}
\end{equation}
In this functional SDE, $\Gamma(H)(t)$ depends on the entire past up to time $t$, i.e. depends on $H_s, 0 \leq s \leq t$. We will rewrite this equation to make its functional nature explicit. (This is based on \cite[Chapter 5]{Mao97} ). Before we do that, so that the domain of our coefficient functions will be consistent, we define the past history of a process $H$ at time $t$ to be the process $\tilde{H}_t \in C[-T, 0]$ given by:
\begin{equation}
\tilde{H}_t(r) = \{H_i(t + r), r \in [-T, 0] \}
\end{equation}
where we take $H_i(s) = 0$ for $s < 0$. Then we can define the new coefficient functions by:
\begin{align*}
\tilde{b}&: C[-T, 0] \rightarrow \mathbb{R}^d && \tilde{b}_i(Y_t) = b( \Gamma(Y_t)(0)) \\
\tilde{\sigma}&: C[-T, 0] \rightarrow \mathbb{R}^d \times \mathbb{R}^d && \tilde{\sigma}(Y_t) = \sigma(\Gamma(Y_t)(0))
\end{align*}
Which lets us rewrite the system as:
\begin{equation} \label{eq:fSDE}
H_i(t) =  \underbrace{ \int_0^t \tilde{b}_i(\tilde{H}_s) ds}_{A_i(t)}+ \underbrace{\sum_{j=1}^d \int_0^t  \tilde{\sigma}_{ij}(\tilde{H}_s)dW_s }_{M_i(t)}
\end{equation}
By construction, $\tilde{b}_i(\tilde{H}_s)$ and $\tilde{\sigma}_{ij}(\tilde{H}_s)$ are $F_s$-measurable (this uses the fact that the Skorokhod map only depends on the past; although this is essentially clear, would like to find a reference for this.) \\ \\

We will consider two cases for the reflecting boundary conditions. First, in the case where U is a convex polygon with nonempty interior and the reflection directions are normal, there is a unique solution to the Skorokhod problem, and the Skorokhod map $\Gamma$ is Lipschitz continuous \cite{Dupuis91}. This can be extended to oblique reflections when U is a convex polygon and the reflection directions are constant on each face and satisfy conditions (look these up!) \cite{Dupuis91}. In the case of normal reflection inside a general smooth, open domain, Lions and Sznitman \cite[Theorem 1.1]{Lions84} showed that there is again a unique solution to the Skorokhod problem, but that the Skohokhod map $\Gamma$ is only Holder continuous of order 1/2 on $[0, T]$.

\subsection{Case 1: Lipschitz continuity of Skorokhod map}
First, consider the case where we have a unique solution to the Skorokhod problem, and the map $\Gamma$ is Lipschitz continuous. Recall that the Skorohod map is a map $\Gamma: C[0, T] \rightarrow C[0, T]$, where we are using the $L^\infty$ norm on $C[0, T]$. Let $C_T$ be the Lipschitz constant. Then since $\Gamma(H) = X$, we have:
\[
\sup_{[0, T]} \abs{ X(s) } \leq C_T \sup_{[0, T]} \abs{ (H)(s) }
\]
Since we can``cut off'' any continuous function at $t$ by sending it linearly to 0 on $[t, T]$, this relation holds with the same Lipschitz constant for all $t \in [0, T]$:
\[
\sup_{[0, t]} \abs{ X(s) } \leq C_T \sup_{[0, t]} \abs{ (H)(s) }
\]
Then we have the following set of inequalities concerning the proposition we are trying to prove:
\begin{align*}
\mathbb{P}\left(\sup_{[0, t]} \norm{X_s} \geq \eta \right) &\leq  \mathbb{P}\left(\sup_{[0, t]} C_T\norm{H_s} \geq \eta \right) \\
&= \mathbb{P}\left(\sup_{[0, t]} \norm{H_s} \geq \frac{\eta}{C_T} \right)
\end{align*}
Since all norms on $\mathbb{R}^d$ are equivalent, we will choose the $l^1$ norm.
\begin{align*}
\mathbb{P}\left(\sup_{[0, t]} \norm{X_t} \geq \eta \right) &\leq \mathbb{P}\left(\sup_{[0, t]} \sum_{i=i}^d \abs{H_i(s)} \geq \frac{\eta}{C_T} \right) \\
&\leq \sum_{i=i}^d \mathbb{P}\left(\sup_{[0, t]}  \abs{H_i(s)} \geq \frac{\eta}{C_T d} \right) \\
&\leq \sum_{i=i}^d \mathbb{P}\left(\sup_{[0, t]}  \abs{A_i(s)} + \sup_{[0, t]}  \abs{M_i(s)} \geq \frac{\eta}{C_T d} \right) 
\end{align*}
where $A_i(s)$ and $M_i(s)$ are as defined above. Then we have:
\begin{align*}
\mathbb{P}\left(\sup_{[0, t]} \norm{X_t} \geq \eta \right) &\leq \sum_{i=i}^d \left[ \mathbb{P}\left(\sup_{[0, t]}  \abs{A_i(s)}  \geq \frac{\eta}{2 C_T d} \right) + \mathbb{P}\left(\sup_{[0, t]}  \abs{M_i(s)}  \geq \frac{\eta}{2 C_T d} \right) \right] \\
&\leq \sum_{i=i}^d \left[ \mathbb{P}\left(\sup_{[0, t]}  A_i(s)  \geq \frac{\eta}{2 C_T d} \right) + \mathbb{P}\left(\sup_{[0, t]}  M_i(s) \geq \frac{\eta}{2 C_T d} \right) \right] 
\end{align*}
where in the last line we have removed the absolute value since $A_i(0) = 0$ and $M_i(0) = 0$. We will then bound the two terms on the RHS individually.

\subsubsection{Existence and Uniqueness of Solution to Functional SDE}
To show existence and uniqueness of solutions to \ref{eq:fSDE}, we need to show that the coefficient functions are Lipschitz and satisfy the standard growth condition. For $Y_t, Z_t \in C[-T, 0]$, we have Lipschitz condition:
\begin{align*}
\abs{ \tilde{b}(Z_t) - \tilde{b}(Y_t) } &= \abs{ b(\Gamma(Z_t)(0) - b(\Gamma(Y_t)(0) } \\
&\leq K_1 \abs{ \Gamma(Z_t)(0) - \Gamma(Y_t)(0)} \\
&\leq K_1 \sup_{[-T, 0]} \abs{ \Gamma(Z_t)(s) - \Gamma(Y_t)(s)} \\
&\leq K_1 C_T \sup_{[-T, 0]} \abs{ Z_t(s) - Y_t(s)}
\end{align*}
and growth condition:
\begin{align*}
\abs{ \tilde{b}(Y_t) }^2 &= \abs{ b(\Gamma(Y_t)(0) }^2 \\
&\leq K_2 ( 1 + \abs{ (\Gamma(Y_t)(0) }^2  ) \\
&\leq K_2 (1 + \sup_{[-T, 0]} \abs{ (\Gamma(Y_t)(s) }^2 ) \\
&\leq K_2 ( 1 + C_T^2  \sup_{[-T, 0]} \abs{ Y_t(s) } ^2 ) \\
&\leq \begin{cases}
K_2( 1+  (\sup_{[-T, 0]} \abs{ Y_t(s) }) ^2 ) & \mbox{if }C_T \leq 1 \\
K_2 C_T^2( 1+  (\sup_{[-T, 0]} \abs{ Y_t(s) }) ^2 ) & \mbox{if }C_T > 1 
\end{cases}
\end{align*}
The same is true for the diffusion coefficient function $\sigma$. Thus we have a unique solution $H_t$ to \ref{eq:fSDE} \cite[Ch. 5, Theorem 2.2]{Mao97}, and for this solution we have:
\begin{equation}
\mathbb{E}\int_0^T \abs{H_t}^2 dt < \infty
\end{equation}
(I don't actually have this book, since the library seems to be missing their copy, but I was able to get this from the Google books preview and the list of notation at the beginning.)

\subsubsection{Ordinary Integral}
First we will find a bound the ordinary integral $A(t)$. \\ \\
Case 1: the coefficient function $b(x)$ is bounded. \\ \\
Suppose $\abs{b(x)} \leq K_b$ for all $x$. This implies that $\abs{\tilde{b}(Y_t)} \leq K_b$ for all $Y_t \in C[-T, 0]$. Then we have for all $\lambda > 0$:
\begin{align*}
\mathbb{P}\left(\sup_{[0, t]}  A_i(s)  \geq \frac{\eta}{2 C_T d} \right) &= \mathbb{P}\left(\sup_{[0, t]}  \exp \lambda A_i(s)^2  \geq \exp \lambda \left(\frac{\eta}{2 C_T d}\right)^2 \right) \\
&\leq \mathbb{P}\left(\sup_{[0, t]}  \exp \lambda \left[ \int_0^s \tilde{b}_i(\tilde{H}_r) dr\right]^2  \geq \exp \lambda \left(\frac{\eta}{2 C_T d}\right)^2 \right) \\
&\leq \mathbb{P}\left(\sup_{[0, t]}  \exp \lambda  \int_0^s \tilde{b}_i(\tilde{H}_r)^2 dr  \geq \exp \lambda \left(\frac{\eta}{2 C_T d}\right)^2 \right) \\
&\leq \mathbb{P}\left( \exp \lambda  \int_0^t \tilde{b}_i(\tilde{H}_r)^2 dr  \geq \exp \lambda \left(\frac{\eta}{2 C_T d}\right)^2 \right)
\end{align*}
By Chebyshev's inequality, we have:
\begin{align*}
\mathbb{P}\left(\sup_{[0, t]}  A_i(s)  \geq \frac{\eta}{2 C_T d} \right) &\leq \mathbb{E}\left[ \exp \lambda  \int_0^t \tilde{b}_i(\tilde{H}_r)^2 dr \right] / \exp \lambda \left(\frac{\eta}{2 C_T d}\right)^2 \\
&\leq \exp \left( \lambda K_b^2t \right) \exp \left( -\lambda \left( \frac{\eta}{2 C_T d} \right)^2 \right)\\
&= \exp \left( K_b^2\right) \exp \left( - \frac{\eta^2}{4 C_T^2 d^2 t} \right)
\end{align*}
where in the last line we have chosen $\lambda = 1/t$.

\subsubsection{Exponential Martingale}
We consider the following exponential function for $\lambda > 0$:
\begin{equation} \label{expmg}
Z_i(t) = \exp \left\{ \sum_{j=1}^d \lambda \int_0^t \tilde{\sigma}_{ij}(\tilde{H}_s) dW_j(s) - \frac{1}{2} \lambda^2 \int_0^t  \norm{ \tilde{\sigma}_i(\tilde{H}_s) }^2 ds  \right\}
\end{equation}
Here we take the $l^2$ norm on $\mathbb{R}^d$, which gives us:
\begin{equation} \label{expmg2}
Z_i(t) = \exp \left\{ \sum_{j=1}^d \lambda \int_0^t \tilde{\sigma}_{ij}(\tilde{H}_s) dW_j(s) - \frac{1}{2} \lambda^2 \int_0^t \sum_{j=1}^d \abs{ \tilde{\sigma}_{ij}(\tilde{H}_s) }^2 ds  \right\}
\end{equation}
Following the analysis in \cite[p. 191]{KS91}, we first show that $Z_i(t)$ is a local martingale. By the property that the Skorokhod map $\Gamma$ only depends on the past of a stochastic process, $\tilde{\sigma}_{ij}(\tilde{H}_s)$ is a measurable, $F_s$-adapted process for all $s$. Furthermore for all $j$ we should be able to show:
\[
\mathbb{P}\left( \int_0^T \tilde{\sigma}_{ij}(\tilde{H}_s)^2 ds < \infty \right) = 1
\]
This should follow from the square integrability of the solution $H_t$ (CHECK THIS!) (This is certainly true if $\sigma$ is bounded.)\\

For our analysis, we would like $Z_i(t)$ to be a martingale. \\

Case 1: the coefficient function $\sigma(x)$ is bounded. \\

Suppose $\abs{\sigma_{ij}(x)} \leq K_s$ for all $i, j, x$. Then we can use the Novikov condition \cite[Corr 5.15, p. 199]{KS91}:
\begin{align*}
\mathbb{E}\left[ \exp\left( \frac{1}{2} \int_0^T \norm{\tilde{\sigma}_i(\tilde{H}_s) }^2 ds \right) \right] &= \mathbb{E}\left[ \exp\left( \frac{1}{2} \int_0^T \sum_{j=1}^d \abs{\tilde{\sigma}_{ij}(\tilde{H}_s)}^2 ds \right) \right] \\
&\leq \exp \left( \frac{1}{2} TdK_s^2 \right) < \infty
\end{align*}
Since the Novikov condition is satisfied, $Z_i(t)$ is a martingale in this case.

\subsubsection{Stochastic Integral}
Recall that:
\[
M_i(t) = \sum_{j=1}^d \int_0^t  \tilde{\sigma}_{ij}(\tilde{H}_s)dW_s
\]
$M_i(t)$ is a stochastic integral, and so is a continuous local martingale. The fact that is is a true martingale should from the square integrability of the solution $H_t$ (CHECK THIS!) \\

Since the function $\exp \lambda x$ is convex, $\exp \lambda M_i(t)$ is a submartingale. Thus by the fact that $\exp \lambda x$ is increasing and by Doob's martingale inequality:
\begin{align*}
\mathbb{P}\left(\sup_{[0, t]}  M_i(s) \geq \frac{\eta}{2 C_T d} \right) &\leq \mathbb{P}\left(\sup_{[0, t]}  \exp \left( \lambda M_i(s) \right) \geq \exp \left( \frac{ \lambda \eta}{2 C_T d} \right)  \right) \\
&\leq \mathbb{E} \exp \left( \lambda M_i(t) \right) / \exp \left( \frac{\lambda \eta}{2 C_T d} \right)
\end{align*}


Here we use the exponential martingale $Z_i(t)$ from above. Since we have $\mathbb{E}Z_i(t) = 1$ for all $t$ by the martingale property:
\begin{align*}
1 &= \mathbb{E}  \exp \left\{ \sum_{j=1}^d \lambda \int_0^t \tilde{\sigma}_{ij}(\tilde{H}_s) dW_j(s) - \frac{1}{2} \lambda^2 \int_0^t \sum_{j=1}^d \abs{ \tilde{\sigma}_{ij}(\tilde{H}_s) }^2 ds  \right\} \\
&= \mathbb{E} \exp \left( \lambda M_i(t) \right) \exp \left\{ -\frac{1}{2} \lambda^2 \int_0^t \sum_{j=1}^d \abs{ \tilde{\sigma}_{ij}(\tilde{H}_s) }^2 ds  \right\}
\end{align*}
Case 1: the coefficient function $\sigma(x)$ is bounded. \\

As above, we take $\abs{\sigma_{ij}(x)} \leq K_s$ for all $i, j, x$. Then we have:
\begin{align*}
1 &\geq \mathbb{E} \exp \left( \lambda M_i(t) \right) \exp \left( - \frac{1}{2} \lambda^2 tdK_s^2 \right) \\
\mathbb{E} \exp \left( \lambda M_i(t) \right) &\leq \exp \left( \frac{1}{2} \lambda^2  tdK_s^2 \right) 
\end{align*}
Using this bound above gives us:
\begin{align*}
\mathbb{P}\left(\sup_{[0, t]}  M_i(s) \geq \frac{\eta}{2 C_T d} \right) &\leq \exp \left( \frac{1}{2} \lambda^2  tdK_s^2 \right) / \exp \left( \frac{\lambda \eta}{2 C_T d} \right) \\
&= \exp \left( \frac{1}{2} \lambda^2  tdK_s^2 - \frac{\lambda \eta}{2 C_T d} \right)
\end{align*}
Inside the parentheses on the RHS is a quadratic in $\lambda$, which takes it's minimum value at the vertex $\lambda = \eta / 2 C_T d^2 K_s^2 t$. Substituting this value of $\lambda$ gives us:
\begin{equation}
\mathbb{P}\left(\sup_{[0, t]}  M_i(s) \geq \frac{\eta}{2 C_T d} \right) \leq \exp \left( - \frac{\eta^2}{t} \frac{1}{8 C_T^2 d^3 K_s^2}  \right)
\end{equation}

% references
\begin{thebibliography}{9}

\bibitem{Dupuis91}Dupuis, P. and Ishii, H. (1991) On Lipschitz continuity of the solution
mapping to the Skorokhod problem, with applications, Stochastics and Stochastic Reports,
35:1, 31-62.

\bibitem{KS91}Karatzas, I. and Shreve S. Brownian Motion and Stochastic Calculus (1991) Springer: Graduate Texts in Mathematics, Volume 113

\bibitem{Lions84} Lions, P. L. and Sznitman, A. S. (1984), Stochastic differential equations with reflecting boundary conditions. Comm. Pure Appl. Math., 37: 511–537.

\bibitem{Mao97}Mao, X (1997). Stochastic Differential Equations and Applications, Horwood

\end{thebibliography}

\end{document}